{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1019\n",
      "23067\n",
      "1019\n",
      "550\n",
      "Train labels distribution:  Counter({'12a': 5000, '0': 5000, '4a': 4698, '7a': 3134, '6a': 1853, '3a': 1446, '5': 656, '11a': 295, '10': 128, '8b': 126, '4b': 111, '11b': 100, '7b': 100, '3b': 100, '12b': 100, '9': 99, '8a': 99, '6b': 22})\n",
      "Valid labels distribution:  Counter({nan: 1019})\n",
      "Test labels distribution:  Counter({'0': 137, '6a': 132, '12a': 56, '5': 52, '4a': 37, '7a': 30, '12b': 17, '3a': 15, '11a': 12, '10': 11, '4b': 10, '8b': 10, '8a': 10, '9': 6, '7b': 6, '6b': 3, '11b': 3, '3b': 3})\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, cross_val_predict\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix,roc_auc_score, roc_curve, precision_recall_curve\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils.fixes import signature\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from scipy.sparse import coo_matrix, hstack, vstack\n",
    "from collections import Counter\n",
    "\n",
    "#Read data\n",
    "train_data_file = \"/efs/CONSORT/MLDataset/train_data_withMetamap.csv\"\n",
    "validation_data_file = \"/efs/CONSORT/MLDataset/valid_data_withMetamap.csv\"\n",
    "test_data_file = \"/efs/CONSORT/MLDataset/test_data_withMetamap.csv\"\n",
    "\n",
    "train_data_df = pd.read_csv(train_data_file, encoding = \"latin\")\n",
    "validation_data_df = pd.read_csv(validation_data_file, encoding = \"latin\")\n",
    "test_data_df = pd.read_csv(test_data_file, encoding = \"latin\")\n",
    "\n",
    "train_data_df = train_data_df[[\"PMCID\", \"sentence_id\",\"CONSORT_Item\",\"section\",\"sentence_text\",\"metamap_concepts\",\"metamap_concepts_text\",\"metamap_semantictypes\"]]\n",
    "validation_data_df = validation_data_df[[\"PMCID\", \"sentence_id\",\"CONSORT_Item\",\"section\",\"sentence_text\",\"metamap_concepts\",\"metamap_concepts_text\",\"metamap_semantictypes\"]]\n",
    "test_data_df = test_data_df[[\"PMCID\", \"sentence_id\",\"CONSORT_Item\",\"section\",\"sentence_text\",\"metamap_concepts\",\"metamap_concepts_text\",\"metamap_semantictypes\"]]\n",
    "\n",
    "all_old_data = pd.concat([validation_data_df,test_data_df])\n",
    "\n",
    "#NEW SPLIT FROM HALIL:\n",
    "new_train_data_file = \"/efs/CONSORT/MLDataset/split_train.csv\"\n",
    "new_test_data_file = \"/efs/CONSORT/MLDataset/split_test.csv\"\n",
    "\n",
    "new_train_data_df = pd.read_csv(new_train_data_file, encoding = \"latin\")\n",
    "new_test_data_df = pd.read_csv(new_test_data_file, encoding = \"latin\")\n",
    "\n",
    "new_train_data_df = new_train_data_df[[\"PMCID\", \"sentence_id\",\"text\",\"labels\",\"CONSORT_Item\",\"n_labels\"]]\n",
    "print (len(new_train_data_df))\n",
    "new_test_data_df = new_test_data_df[[\"PMCID\", \"sentence_id\",\"text\",\"labels\",\"CONSORT_Item\",\"n_labels\"]]\n",
    "\n",
    "validation_data_df = pd.merge(new_train_data_df,all_old_data, on = [\"PMCID\",\"sentence_id\"], how = \"left\")\n",
    "print (len(train_data_df))\n",
    "print (len(validation_data_df))\n",
    "validation_data_df = validation_data_df[[\"PMCID\", \"sentence_id\",\"CONSORT_Item_y\",\"section\",\"sentence_text\",\"metamap_concepts\",\"metamap_concepts_text\",\"metamap_semantictypes\"]]\n",
    "validation_data_df.rename(columns = {'CONSORT_Item_y':'CONSORT_Item'}, inplace = True) \n",
    "\n",
    "test_data_df = pd.merge(new_test_data_df,all_old_data, on = [\"PMCID\",\"sentence_id\"], how = \"left\")\n",
    "print (len(test_data_df))\n",
    "test_data_df = test_data_df[[\"PMCID\", \"sentence_id\",\"CONSORT_Item_y\",\"section\",\"sentence_text\",\"metamap_concepts\",\"metamap_concepts_text\",\"metamap_semantictypes\"]]\n",
    "test_data_df.rename(columns = {'CONSORT_Item_y':'CONSORT_Item'}, inplace = True) \n",
    "\n",
    "train_labels = train_data_df[\"CONSORT_Item\"]\n",
    "train_labels = train_labels.tolist()\n",
    "train_value_counts = Counter(train_labels)\n",
    "print (\"Train labels distribution: \", train_value_counts)\n",
    "\n",
    "valid_labels = validation_data_df[\"CONSORT_Item\"]\n",
    "valid_labels = valid_labels.tolist()\n",
    "valid_value_counts = Counter(valid_labels)\n",
    "print (\"Valid labels distribution: \", valid_value_counts)\n",
    "\n",
    "test_labels = test_data_df[\"CONSORT_Item\"]\n",
    "test_labels = test_labels.tolist()\n",
    "test_value_counts = Counter(test_labels)\n",
    "print (\"Test labels distribution: \", test_value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:  (23067, 26518)\n",
      "Validation data:  (2002, 26518)\n",
      "Test data:  (550, 26518)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# TRAINING DATA\n",
    "#Extract features from training data\n",
    "#Get ngram features\n",
    "vectorizer = CountVectorizer(ngram_range = (2,2))\n",
    "train_text_features = vectorizer.fit_transform(train_data_df['sentence_text'])\n",
    "#select the best 2000 bigram features\n",
    "selector = SelectKBest(chi2, k=5000).fit(train_text_features, train_data_df['CONSORT_Item'])\n",
    "train_text_features_selected = selector.transform(train_text_features) \n",
    "\n",
    "# Get section header feature\n",
    "lb_make = LabelBinarizer()\n",
    "train_header_features = lb_make.fit_transform(train_data_df[\"section\"].astype(str))\n",
    "train_header_features = np.asmatrix(train_header_features)\n",
    "\n",
    "metamap_concepts_unique = []\n",
    "train_metamap_concepts = list(train_data_df['metamap_concepts'])\n",
    "for concepts in train_metamap_concepts:\n",
    "    if pd.notna(concepts) :\n",
    "        concept_items  = concepts.split(\"|\")\n",
    "        for concept_item in concept_items:\n",
    "            if concept_item not in metamap_concepts_unique:\n",
    "                metamap_concepts_unique.append(concept_item)\n",
    "                \n",
    "metamap_semantictypes_unique = []\n",
    "train_metamap_semantictypes = list(train_data_df['metamap_semantictypes'])\n",
    "for semantictypes in train_metamap_semantictypes:\n",
    "    if pd.notna(semantictypes) :\n",
    "        semantictypes_items  = semantictypes.split(\"|\")\n",
    "        for semantictypes_item in semantictypes_items:\n",
    "            if semantictypes_item not in metamap_semantictypes_unique:\n",
    "                metamap_semantictypes_unique.append(semantictypes_item)\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=metamap_concepts_unique)\n",
    "train_metamap_concepts_features = mlb.fit_transform(train_data_df[\"metamap_concepts\"].astype(str))\n",
    "\n",
    "mlb_semantictype = MultiLabelBinarizer(classes=metamap_semantictypes_unique)\n",
    "train_semantictype_features = mlb_semantictype.fit_transform(train_data_df[\"metamap_semantictypes\"].astype(str))\n",
    "\n",
    "# X_train_dm = train_text_features_selected\n",
    "# X_train_dm = hstack([train_text_features_selected,train_header_features])\n",
    "X_train_dm = hstack([train_text_features_selected,train_metamap_concepts_features,train_semantictype_features])\n",
    "print (\"Training data: \" , X_train_dm.shape)\n",
    "y_train_dm = train_data_df['CONSORT_Item']\n",
    "\n",
    "#VALIDATION DATA\n",
    "valid_text = validation_data_df['sentence_text']\n",
    "valid_text_features = vectorizer.transform(valid_text)\n",
    "valid_text_features_selected = selector.transform(valid_text_features) \n",
    "valid_header_features = lb_make.transform(validation_data_df[\"section\"])\n",
    "valid_header_features = np.asmatrix(valid_header_features)\n",
    "valid_metamap_concepts_features = mlb.transform(validation_data_df[\"metamap_concepts\"].astype(str))\n",
    "valid_semantictype_features = mlb_semantictype.transform(validation_data_df[\"metamap_semantictypes\"].astype(str))\n",
    "# X_valid_dm = valid_text_features_selected\n",
    "# X_valid_dm = hstack([valid_text_features_selected,valid_header_features])\n",
    "X_valid_dm = hstack([valid_text_features_selected,valid_metamap_concepts_features,valid_semantictype_features])\n",
    "print (\"Validation data: \", X_valid_dm.shape)\n",
    "y_valid_dm = validation_data_df['CONSORT_Item']\n",
    "\n",
    "# #TESTING DATA \n",
    "test_text = test_data_df['sentence_text']\n",
    "test_text_features = vectorizer.transform(test_text)\n",
    "test_text_features_selected = selector.transform(test_text_features) \n",
    "test_header_features = lb_make.transform(test_data_df[\"section\"])\n",
    "test_header_features = np.asmatrix(test_header_features)\n",
    "test_metamap_concepts_features = mlb.transform(test_data_df[\"metamap_concepts\"].astype(str))\n",
    "test_semantictype_features = mlb_semantictype.transform(test_data_df[\"metamap_semantictypes\"].astype(str))\n",
    "# X_test_dm = test_text_features_selected\n",
    "# X_test_dm = hstack([test_text_features_selected,test_header_features])\n",
    "X_test_dm = hstack([test_text_features_selected,test_metamap_concepts_features,test_semantictype_features])\n",
    "print (\"Test data: \", X_test_dm.shape)\n",
    "y_test_dm = test_data_df['CONSORT_Item']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0]))\n",
      "(25069, 26518)\n",
      "(25069,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "import numpy\n",
    "\n",
    "X_train = vstack([X_train_dm, X_valid_dm])\n",
    "y_train = y_train_dm.append(y_valid_dm)\n",
    "\n",
    "\n",
    "test_fold = numpy.concatenate([\n",
    "    # The training data.\n",
    "    numpy.full(X_train_dm.shape[0],-1, dtype=numpy.int8),\n",
    "    # The development data.\n",
    "    numpy.zeros(X_valid_dm.shape[0], dtype=numpy.int8)\n",
    "])\n",
    "\n",
    "cv = PredefinedSplit(test_fold)\n",
    "print (cv)\n",
    "print (X_train.shape)\n",
    "print (y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 2 candidates, totalling 2 fits\n",
      "[CV] C=1, gamma=1, kernel=linear .....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...................... C=1, gamma=1, kernel=linear, total=  19.4s\n",
      "[CV] C=10, gamma=1, kernel=linear ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   41.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... C=10, gamma=1, kernel=linear, total=  28.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training process...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.svm import LinearSVC\n",
    "import pickle\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# chi_squarer = SelectKBest(chi2, k=5000)  \n",
    "\n",
    "# Choose some parameter combinations to try\n",
    "param_grid = {'C':[1,10],'gamma':[1], 'kernel':['linear']}\n",
    "clf = SVC(decision_function_shape = \"ovr\")\n",
    "\n",
    "# Run the grid search\n",
    "grid_obj = GridSearchCV(clf,param_grid,refit = True, verbose=2,cv=cv)\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Set the clf to the best combination of parameters\n",
    "clf = grid_obj.best_estimator_\n",
    "\n",
    "#this is the classifier used for feature selection\n",
    "clf_pipe_multiclass = Pipeline([('model', clf)])\n",
    "\n",
    "# Fit the best algorithm to the data\n",
    "print (\"Start training process...\")\n",
    "clf_multiclass = clf.fit(X_train, y_train)\n",
    "\n",
    "# # save the models to disk\n",
    "# filename_classifier = '/efs/lhoang2/Models/model_sentenceTextOnly.sav'\n",
    "# pickle.dump(clf, open(filename_classifier, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5054545454545455\n",
      "Precision: 0.5919060968059979\n",
      "Recall: 0.5054545454545455\n",
      "F1 score: 0.47644005074445556\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.82      0.52       137\n",
      "          10       1.00      0.18      0.31        11\n",
      "         11a       0.71      0.42      0.53        12\n",
      "         11b       0.33      0.33      0.33         3\n",
      "         12a       0.59      0.73      0.66        56\n",
      "         12b       0.67      0.12      0.20        17\n",
      "          3a       0.45      0.60      0.51        15\n",
      "          3b       0.00      0.00      0.00         3\n",
      "          4a       0.76      0.59      0.67        37\n",
      "          4b       1.00      0.10      0.18        10\n",
      "           5       0.55      0.31      0.40        52\n",
      "          6a       0.68      0.24      0.36       132\n",
      "          6b       0.00      0.00      0.00         3\n",
      "          7a       0.92      0.73      0.81        30\n",
      "          7b       0.75      1.00      0.86         6\n",
      "          8a       0.50      0.10      0.17        10\n",
      "          8b       0.31      0.40      0.35        10\n",
      "           9       0.67      0.33      0.44         6\n",
      "\n",
      "   micro avg       0.51      0.51      0.51       550\n",
      "   macro avg       0.57      0.39      0.41       550\n",
      "weighted avg       0.59      0.51      0.48       550\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# save the models to disk\n",
    "filename_classifier = '/efs/lhoang2/Models/model_sentenceText_MetaMapConcepts_MetamapSemanticTypes_LimitedBigram.sav'\n",
    "pickle.dump(clf_multiclass, open(filename_classifier, 'wb'))\n",
    "\n",
    "#Test SET\n",
    "#Run the model on Test set\n",
    "X_test = X_test_dm\n",
    "y_test = y_test_dm\n",
    "# Get validation results\n",
    "predictions_multiclass = clf_multiclass.predict(X_test)\n",
    "#Print Accurancy, ROC AUC, F1 Scores, Recall, Precision)\n",
    "print ('Accuracy:', accuracy_score(y_test, predictions_multiclass))\n",
    "print ('Precision:', precision_score(y_test, predictions_multiclass,average='weighted'))\n",
    "print ('Recall:', recall_score(y_test, predictions_multiclass,average='weighted'))\n",
    "print ('F1 score:', f1_score(y_test, predictions_multiclass,average='weighted'))\n",
    "print (classification_report(y_test,predictions_multiclass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
